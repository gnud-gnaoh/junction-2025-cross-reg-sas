{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2430184f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd6fb23",
   "metadata": {},
   "source": [
    "### Pre-process data and clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cd4790a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\danit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\danit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\danit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading & Cleaning...\n",
      "Error: File not found at final_data/final_combined_classified_data.csv\n",
      "Starting text preprocessing (this may take a while)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Text: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 66227/66227 [00:41<00:00, 1596.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete in 41.49 seconds.\n",
      "                                           Paragraph  \\\n",
      "0  1) mortgage credit bank operations mean activi...   \n",
      "1  11) cover pool means a set of collaterals for ...   \n",
      "2  12) Resolution Authority means the Finnish Fin...   \n",
      "3  In addition to what is provided in subsection ...   \n",
      "4  In addition to what is provided in the Act on ...   \n",
      "\n",
      "                                   cleaned_paragraph  \n",
      "0  mortgage credit bank operation mean activity c...  \n",
      "1  cover pool mean set collateral one specified c...  \n",
      "2  resolution authority mean finnish financial st...  \n",
      "3  addition provided subsection mortgage credit b...  \n",
      "4  addition provided act credit institution suita...  \n",
      "\n",
      "Vectorization (TF-IDF)\n",
      "Vectorizing 66227 paragraphs...\n",
      "Vectorization complete in 3.93 seconds.\n",
      "TF-IDF Matrix Shape: (paragraphs, features) = (66227, 1000)\n",
      "\n",
      "--- Step 3: Exploratory Clustering (K-Means) ---\n",
      "Starting K-Means clustering with k=25...\n",
      "Clustering complete in 26.58 seconds.\n",
      "Model Inertia (lower is better): 57864.87391730212\n",
      "\n",
      "Step 4: Manual Inspection\n",
      "\n",
      "Review Top Words per Cluster\n",
      "Cluster 0: data, personal, processing, information, right, protection, regulation, subject, article, access\n",
      "Cluster 1: eur, million, billion, loan, capital, value, cost, total, guarantee, aid\n",
      "Cluster 2: credit, exposure, risk, institution, default, rating, counterparty, shall, approach, collateral\n",
      "Cluster 3: aid, commission, state, breach, article, granted, measure, treaty, restructuring, market\n",
      "Cluster 4: commission, company, market, party, authority, information, financial, measure, cost, decision\n",
      "Cluster 5: union, industry, producer, production, indicator, injury, price, import, capacity, macroeconomic\n",
      "Cluster 6: service, provider, ict, public, payment, provision, contract, activity, outsourcing, financial\n",
      "Cluster 7: flow, cash, investment, profitability, return, period, union, ability, producer, net\n",
      "Cluster 8: union, european, strategy, council, support, international, objective, eu, development, implementation\n",
      "Cluster 9: member, state, union, authority, national, crossborder, measure, regulation, law, commission\n",
      "Cluster 10: article, shall, referred, regulation, paragraph, accordance, point, authority, eu, following\n",
      "Cluster 11: lgd, downturn, estimate, exposure, estimation, institution, approach, default, collateral, period\n",
      "Cluster 12: institution, guideline, risk, competent, authority, credit, requirement, internal, financial, eba\n",
      "Cluster 13: remuneration, staff, variable, capital, commission, paid, tax, policy, institution, instrument\n",
      "Cluster 14: rate, exchange, currency, bond, loan, risk, benchmark, market, average, value\n",
      "Cluster 15: emission, gas, greenhouse, directive, eu, fuel, european, union, allowance, energy\n",
      "Cluster 16: derivative, trading, contract, clearing, otc, transaction, market, commodity, instrument, position\n",
      "Cluster 17: entity, supervised, chapter, act, section, customer, obliged, shall, information, person\n",
      "Cluster 18: management, body, function, member, institution, guideline, internal, risk, holder, key\n",
      "Cluster 19: growth, fiscal, programme, gdp, reform, economic, stability, europe, deficit, policy\n",
      "Cluster 20: european, parliament, eu, council, directive, regulation, oj, amending, councilregulation, ec\n",
      "Cluster 21: nation, person, united, list, sanction, committee, decided, security, entity, resource\n",
      "Cluster 22: regulation, eu, implementing, commission, article, ec, delegated, annex, council, european\n",
      "Cluster 23: bank, central, capital, risk, restructuring, asset, supervisor, deposit, loan, financial\n",
      "Cluster 24: risk, management, institution, model, requirement, assessment, operational, credit, market, factor\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize tqdm for pandas\n",
    "# This allows us to use .progress_apply() for a progress bar\n",
    "tqdm.pandas(desc=\"Cleaning Text\")\n",
    "\n",
    "# Download NLTK data (only need to run this once) \n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Pre-processing (Load & Clean)\n",
    "\n",
    "print(\"Loading & Cleaning...\")\n",
    "start_load = time.time()\n",
    "\n",
    "# Load your CSV\n",
    "file_path = 'final_data/final_combined_classified_data.csv'  \n",
    "try:\n",
    "    df = pd.read_csv(file_path, sep='|', on_bad_lines='skip')\n",
    "    end_load = time.time()\n",
    "    print(f\"Loaded {len(df)} rows in {end_load - start_load:.2f} seconds.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {file_path}\")\n",
    "\n",
    "# Initialize cleaning tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Cleans and tokenizes text.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Remove stop words and lemmatize\n",
    "    clean_tokens = [\n",
    "        lemmatizer.lemmatize(word) \n",
    "        for word in tokens \n",
    "        if word.isalpha() and word not in stop_words\n",
    "    ]\n",
    "    return ' '.join(clean_tokens)\n",
    "\n",
    "print(\"Starting text preprocessing (this may take a while)...\")\n",
    "start_clean = time.time()\n",
    "# Use .progress_apply() to see a progress bar\n",
    "df['cleaned_paragraph'] = df['Paragraph'].progress_apply(preprocess_text)\n",
    "end_clean = time.time()\n",
    "print(f\"Preprocessing complete in {end_clean - start_clean:.2f} seconds.\")\n",
    "print(df[['Paragraph', 'cleaned_paragraph']].head())\n",
    "\n",
    "# Vectorization (TF-IDF) \n",
    "print(\"\\nVectorization (TF-IDF)\")\n",
    "start_vec = time.time()\n",
    "\n",
    "# Tuned these parameters. 100 features was too low.\n",
    "vectorizer = TfidfVectorizer(max_df=0.90, min_df=5, stop_words='english', max_features=1000)\n",
    "\n",
    "# Filter out empty paragraphs that might result from cleaning\n",
    "original_rows = len(df)\n",
    "non_empty_df = df[df['cleaned_paragraph'].str.strip().astype(bool)].copy()\n",
    "rows_dropped = original_rows - len(non_empty_df)\n",
    "\n",
    "if rows_dropped > 0:\n",
    "    print(f\"Filtered {rows_dropped} empty/unusable rows.\")\n",
    "\n",
    "if non_empty_df.empty:\n",
    "    print(\"Error: No data left after cleaning or all paragraphs were shorter than min_df.\")\n",
    "else:\n",
    "    print(f\"Vectorizing {len(non_empty_df)} paragraphs...\")\n",
    "    tfidf_matrix = vectorizer.fit_transform(non_empty_df['cleaned_paragraph'])\n",
    "    end_vec = time.time()\n",
    "    print(f\"Vectorization complete in {end_vec - start_vec:.2f} seconds.\")\n",
    "    print(f\"TF-IDF Matrix Shape: (paragraphs, features) = {tfidf_matrix.shape}\")\n",
    "\n",
    "    # Exploratory Clustering (K-Means)\n",
    "    print(\"\\n--- Step 3: Exploratory Clustering (K-Means) ---\")\n",
    "    start_cluster = time.time()\n",
    "    \n",
    "    # We pick a high-ish 'k' to find junk topics\n",
    "    k_exploratory = 25 \n",
    "    \n",
    "    # Ensure k is not larger than the number of samples\n",
    "    if k_exploratory > tfidf_matrix.shape[0]:\n",
    "        k_exploratory = tfidf_matrix.shape[0]\n",
    "        print(f\"Warning: k was larger than sample size. Setting k to {k_exploratory}.\")\n",
    "\n",
    "    print(f\"Starting K-Means clustering with k={k_exploratory}...\")\n",
    "    kmeans = KMeans(n_clusters=k_exploratory, random_state=42, n_init=10)\n",
    "    kmeans.fit(tfidf_matrix)\n",
    "    end_cluster = time.time()\n",
    "    \n",
    "    # Add cluster labels back to our non-empty DataFrame\n",
    "    non_empty_df['cluster_id'] = kmeans.labels_\n",
    "    print(f\"Clustering complete in {end_cluster - start_cluster:.2f} seconds.\")\n",
    "    print(f\"Model Inertia (lower is better): {kmeans.inertia_}\")\n",
    "\n",
    "\n",
    "    # Manual Inspection (Review Keywords)\n",
    "    print(\"\\nStep 4: Manual Inspection\")\n",
    "    print(\"\\nReview Top Words per Cluster\")\n",
    "    \n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "    \n",
    "    for i in range(k_exploratory):\n",
    "        top_words = [terms[ind] for ind in order_centroids[i, :10]]\n",
    "        print(f\"Cluster {i}: {', '.join(top_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f67edb",
   "metadata": {},
   "source": [
    "### Remove paragraphs containing certain keywords from clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c09b946",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filtering \n",
    "print(\"\\nFiltering \")\n",
    "\n",
    "# Indices of clusters to remove\n",
    "clusters_to_remove = []\n",
    "\n",
    "if not clusters_to_remove:\n",
    "    print(\"NOTE: `clusters_to_remove` is empty. No filtering will be applied.\")\n",
    "\n",
    "# Filter the DataFrame\n",
    "clean_df = non_empty_df[~non_empty_df['cluster_id'].isin(clusters_to_remove)].copy()\n",
    "\n",
    "print(\"Saving filtered data to CSV...\")\n",
    "start_save = time.time()\n",
    "save_path = '../filtered_data/filtered_paragraphs_3.csv'\n",
    "clean_df.to_csv(save_path, index=False, sep='|')\n",
    "end_save = time.time()\n",
    "print(f\"Filtered data saved to {save_path} in {end_save - start_save:.2f} seconds.\")\n",
    "\n",
    "print(\"\\nFiltering complete.\")\n",
    "print(f\"Original non-empty paragraphs: {len(non_empty_df)}\")\n",
    "print(f\"Paragraphs removed: {len(non_empty_df) - len(clean_df)}\")\n",
    "print(f\"Clean paragraphs remaining: {len(clean_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4554f082",
   "metadata": {},
   "source": [
    "### Classify each paragraph into financial risk classes based on keywords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ae4fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure tqdm is set up for pandas\n",
    "tqdm.pandas(desc=\"Classifying Risks\")\n",
    "\n",
    "# Define all Term Lists (Expanded Version) \n",
    "\n",
    "# This dictionary holds all keyword lists\n",
    "term_lists = {\n",
    "    \"RiskTriggerTerms\": [\"risk\", \"risks\", \"exposure\", \"exposures\", \"vulnerability\", \"vulnerabilities\", \"hazard\", \"threat\", \"peril\"],\n",
    "    \n",
    "    \"DefaultTerms\": [\"default\", \"defaults\", \"non-performing\", \"non performing\", \"past due\", \"delinquency\", \"delinquent\", \n",
    "                     \"arrears\", \"insolvency\", \"bankruptcy\", \"foreclosure\", \"write-off\", \"charge-off\", \"impaired loan\", \"credit loss\"],\n",
    "                     \n",
    "    \"CreditTerms\": [\"credit\", \"loan\", \"loans\", \"lending\", \"obligor\", \"borrower\", \"debtor\", \"mortgage\", \"creditor\", \n",
    "                    \"debenture\", \"bond\", \"overdraft\", \"facility\", \"credit line\", \"loan portfolio\", \"credit institution\"],\n",
    "                    \n",
    "    \"CounterpartyTerms\": [\"counterparty\", \"counterparties\", \"trading partner\", \"derivative counterparty\", \"clearing member\", \n",
    "                          \"CCP\", \"central counterparty\", \"clearing house\", \"bilateral agreement\", \"ISDA\"],\n",
    "                          \n",
    "    \"CollateralTerms\": [\"collateral\", \"security\", \"securities\", \"pledge\", \"guarantee\", \"guarantor\", \"margin\", \n",
    "                        \"encumbrance\", \"lien\", \"charge\", \"hypothecation\", \"repo\", \"reverse repo\", \"security interest\", \"property\"],\n",
    "                        \n",
    "    \"CountryTerms\": [\"country risk\", \"transfer risk\", \"sovereign risk\", \"cross-border\", \"cross border\", \"geopolitical risk\", \"sovereign default\"],\n",
    "    \n",
    "    \"MarketTerms\": [\"market risk\", \"market volatility\", \"price volatility\", \"spread risk\", \"trading book\", \"equities\", \"bonds\", \n",
    "                    \"commodities\", \"derivatives\", \"spreads\", \"VaR\", \"value at risk\", \"expected shortfall\", \"ES\", \"mark-to-market\", \"MTM\"],\n",
    "                    \n",
    "    \"InterestRateTerms\": [\"interest rate risk\", \"IRRBB\", \"yield curve\", \"repricing gap\", \"net interest income\", \"NII\", \n",
    "                          \"economic value of equity\", \"EVE\", \"basis risk\", \"yield curve risk\", \"repricing risk\"],\n",
    "                          \n",
    "    \"LiquidityTerms\": [\"liquidity risk\", \"funding liquidity\", \"market liquidity\", \"liquidity coverage ratio\", \"LCR\", \"NSFR\", \n",
    "                       \"net stable funding ratio\", \"liquidity buffer\", \"cash flow\", \"funding mismatch\", \"run-off\", \"deposit outflow\", \n",
    "                       \"HQLA\", \"high-quality liquid assets\", \"ILAAP\", \"contingency funding plan\", \"CFP\"],\n",
    "                       \n",
    "    \"FXTerms\": [\"foreign exchange\", \"FX risk\", \"currency mismatch\", \"exchange rate\", \"currency risk\", \"FX exposure\", \n",
    "                \"translation risk\", \"transaction risk\", \"foreign currency risk\"],\n",
    "                \n",
    "    \"CommodityTerms\": [\"commodity price\", \"commodity risk\", \"oil price\", \"gas price\", \"energy price\", \"metal price\", \"agricultural price\"],\n",
    "    \n",
    "    \"OperationalTerms\": [\"operational risk\", \"operational failure\", \"internal process\", \"process failure\", \"error\", \"manual error\", \n",
    "                         \"oprisk\", \"RCSA\", \"risk and control self-assessment\", \"loss event\", \"failed process\", \"people risk\", \n",
    "                         \"business disruption\", \"BCP\", \"business continuity\", \"internal fraud\", \"external fraud\"],\n",
    "                         \n",
    "    \"ITTerms\": [\"IT system\", \"system outage\", \"system failure\", \"downtime\", \"IT disruption\", \"legacy system\", \"core banking system\", \n",
    "                \"infrastructure failure\", \"network outage\", \"disaster recovery\", \"DR\", \"technology failure\", \"platform instability\"],\n",
    "                \n",
    "    \"CyberTerms\": [\"cyber risk\", \"cybersecurity\", \"cyber attack\", \"malware\", \"ransomware\", \"data breach\", \"infosec\", \n",
    "                   \"information security\", \"phishing\", \"vishing\", \"smishing\", \"denial of service\", \"DoS\", \"DDoS\", \n",
    "                   \"zero-day\", \"exploit\", \"unauthorized access\", \"cyber threat\", \"cyber incident\"],\n",
    "                   \n",
    "    \"ModelTerms\": [\"risk model\", \"scoring model\", \"PD model\", \"LGD model\", \"EAD model\", \"model validation\", \"model risk\", \n",
    "                   \"model uncertainty\", \"model error\", \"model overlay\", \"back-testing\", \"backtesting\", \"model governance\", \n",
    "                   \"model implementation\", \"model misuse\", \"pd\", \"probability of default\", \"lgd\", \"loss given default\", \"estimation\", \"downturn\"],\n",
    "                   \n",
    "    \"DataQualityTerms\": [\"data quality\", \"data integrity\", \"data accuracy\", \"data completeness\", \"data inconsistency\", \n",
    "                         \"data governance\", \"data lineage\", \"data validation\", \"reconciliation\", \"inaccurate data\", \n",
    "                         \"incomplete data\", \"data stewardship\", \"data reliability\"],\n",
    "                         \n",
    "    \"OutsourcingTerms\": [\"outsourcing\", \"third-party\", \"third party\", \"external service provider\", \"cloud service provider\", \n",
    "                         \"fourth-party\", \"sub-contractor\", \"supply chain risk\", \"vendor risk\", \"service level agreement\", \"SLA\", \n",
    "                         \"vendor management\", \"supplier risk\"],\n",
    "                         \n",
    "    \"LegalTerms\": [\"legal risk\", \"enforceability\", \"legal enforceability\", \"litigation\", \"legal dispute\", \"non-enforceable\", \n",
    "                   \"unenforceable\", \"contract risk\", \"legal uncertainty\", \"regulatory action\", \"lawsuit\", \"arbitration\"],\n",
    "                   \n",
    "    \"ComplianceTerms\": [\"compliance risk\", \"regulatory requirements\", \"supervisory expectations\", \"prudential requirements\", \n",
    "                        \"regulatory reporting\", \"non-compliance\", \"breach\", \"regulatory breach\", \"supervisory action\", \n",
    "                        \"regulatory fine\", \"regulatory scrutiny\", \"supervisory review\"],\n",
    "                        \n",
    "    \"AMLTerms\": [\"money laundering\", \"terrorist financing\", \"AML\", \"CFT\", \"sanctions\", \"sanctioned entity\", \"KYC\", \n",
    "                 \"know your customer\", \"PEP\", \"politically exposed person\", \"SAR\", \"suspicious activity report\", \n",
    "                 \"OFAC\", \"financial crime\", \"fatf\", \"financial action task force\", \"mltf\"],\n",
    "                 \n",
    "    \"ConductTerms\": [\"conduct risk\", \"mis-selling\", \"mis selling\", \"consumer protection\", \"treating customers fairly\", \"TCF\", \n",
    "                     \"product suitability\", \"client detriment\", \"conflict of interest\", \"insider trading\", \"market abuse\", \"market manipulation\"],\n",
    "                     \n",
    "    \"ReputationTerms\": [\"reputational risk\", \"reputation risk\", \"adverse publicity\", \"public perception\", \"media coverage\", \n",
    "                        \"brand damage\", \"public trust\", \"scandal\", \"negative press\", \"stakeholder perception\", \"loss of trust\"],\n",
    "                        \n",
    "    \"GovernanceTerms\": [\"governance\", \"internal control\", \"risk committee\", \"board oversight\", \"risk culture\", \"conflict of interest\", \n",
    "                        \"risk appetite\", \"three lines of defense\", \"3LOD\", \"audit committee\", \"remuneration\", \"corporate governance\", \n",
    "                        \"board of directors\", \"management body\", \"accountability\"],\n",
    "                        \n",
    "    \"StrategicTerms\": [\"strategic risk\", \"business model\", \"strategy\", \"business strategy\", \"competitive risk\", \"business environment\", \n",
    "                       \"merger\", \"acquisition\", \"M&A\", \"business model viability\", \"competitive landscape\"],\n",
    "                       \n",
    "    \"SystemicTerms\": [\"systemic risk\", \"macroprudential\", \"contagion\", \"spillover\", \"too big to fail\", \"interconnectedness\", \n",
    "                      \"financial stability\", \"domino effect\", \"G-SIB\", \"G-SII\", \"globally systemically important\"],\n",
    "                      \n",
    "    \"ClimateTerms\": [\"climate risk\", \"environmental risk\", \"physical risk\", \"transition risk\", \"ESG risk\", \"greenhouse gas\", \n",
    "                     \"sustainability\", \"carbon footprint\", \"stranded assets\", \"TCFD\", \"greenwashing\", \"environmental social governance\"],\n",
    "                     \n",
    "    \"MacroTerms\": [\"macroeconomic\", \"business cycle\", \"economic downturn\", \"recession\", \"inflation shock\", \"geopolitical\", \n",
    "                   \"interest rate shock\", \"stagflation\", \"unemployment\", \"GDP growth\", \"economic shock\", \"inflationary pressure\"],\n",
    "                   \n",
    "    \"RequirementTerms\": [\"requirement\", \"requirements\", \"obligation\", \"obligations\", \"shall\", \"must\", \"binding\", \"mandatory\"],\n",
    "    \n",
    "    \"CapitalTerms\": [\"capital requirement\", \"capital buffer\", \"Pillar 2\", \"own funds\", \"capital adequacy\", \"CET1\", \"Common Equity Tier 1\", \n",
    "                     \"Tier 1 capital\", \"Tier 2 capital\", \"T1\", \"T2\", \"Basel III\", \"Basel 3\", \"Basel IV\", \"Basel 4\", \"ICAAP\", \n",
    "                     \"leverage ratio\", \"RWA\", \"risk-weighted assets\", \"capital ratio\", \"Pillar 1\", \"irb\", \"internal ratings based\", \"standardized approach\", \"securisation\", \"asset\", \"liability\", \"equity\", \"share\", \"tier\"]\n",
    "}\n",
    "\n",
    "# Helper Functions for Rule Logic\n",
    "\n",
    "def build_regex_pattern(terms):\n",
    "    \"\"\"Builds a regex pattern from a list of terms.\"\"\"\n",
    "    # We sort by length, longest first, to match \"non performing\" before \"non\"\n",
    "    sorted_terms = sorted(terms, key=len, reverse=True)\n",
    "    # Create pattern: \\b(term1|term2|...)\\b\n",
    "    return r'\\b(' + '|'.join(re.escape(term) for term in sorted_terms) + r')\\b'\n",
    "\n",
    "def check_proximity(text, terms1_list, terms2_list, distance):\n",
    "    \"\"\"\n",
    "    Checks if a term from terms1 is within 'distance' words of a term from terms2.\n",
    "    \"\"\"\n",
    "    # Create regex patterns for both lists\n",
    "    pattern1 = build_regex_pattern(terms1_list)\n",
    "    pattern2 = build_regex_pattern(terms2_list)\n",
    "    \n",
    "    # Create forward and backward proximity patterns\n",
    "    # (A w/N B) -> A ... B\n",
    "    forward_pattern = pattern1 + (r'(?:\\s+\\w+){0,' + str(distance) + r'}\\s+') + pattern2\n",
    "    # (B w/N A) -> B ... A\n",
    "    backward_pattern = pattern2 + (r'(?:\\s+\\w+){0,' + str(distance) + r'}\\s+') + pattern1\n",
    "\n",
    "    # Check if either pattern exists\n",
    "    if re.search(forward_pattern, text, re.IGNORECASE) or \\\n",
    "       re.search(backward_pattern, text, re.IGNORECASE):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def check_list(text, terms_list):\n",
    "    \"\"\"Checks if any term from the list exists in the text.\"\"\"\n",
    "    pattern = build_regex_pattern(terms_list)\n",
    "    if re.search(pattern, text, re.IGNORECASE):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# The Main Classifier Function \n",
    "\n",
    "def classify_risk_concept(text):\n",
    "    \"\"\"\n",
    "    Classifies text based on the 30-rule concept model.\n",
    "    Uses a 'first match wins' logic.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"Unclassified\"\n",
    "    \n",
    "    # 1. Risk_CreditDefault\n",
    "    if check_proximity(text, term_lists[\"CreditTerms\"], term_lists[\"DefaultTerms\"], 3) or \\\n",
    "       check_proximity(text, term_lists[\"DefaultTerms\"], [\"exposure\"], 3) or \\\n",
    "       check_proximity(text, term_lists[\"RiskTriggerTerms\"], term_lists[\"DefaultTerms\"], 3) or \\\n",
    "       re.search(r'credit default risk|risk of default', text, re.IGNORECASE):\n",
    "        return \"Risk_CreditDefault\"\n",
    "\n",
    "    # 2. Risk_CounterpartyCredit\n",
    "    if check_proximity(text, term_lists[\"CounterpartyTerms\"], term_lists[\"DefaultTerms\"], 5) or \\\n",
    "       check_proximity(text, term_lists[\"CounterpartyTerms\"], [\"credit risk\"], 5) or \\\n",
    "       check_proximity(text, [\"settlement\"], term_lists[\"CounterpartyTerms\"], 5) or \\\n",
    "       re.search(r'counterparty credit risk', text, re.IGNORECASE):\n",
    "        return \"Risk_CounterpartyCredit\"\n",
    "    \n",
    "    # 3. Risk_Concentration\n",
    "    if re.search(r'concentration risk|large exposure|large exposures', text, re.IGNORECASE) or \\\n",
    "       check_proximity(text, [\"sectoral\"], [\"concentration\"], 3) or \\\n",
    "       check_proximity(text, [\"portfolio\"], [\"concentration\"], 3):\n",
    "        return \"Risk_Concentration\"\n",
    "\n",
    "    # 4. Risk_CollateralQuality\n",
    "    if check_proximity(text, term_lists[\"CollateralTerms\"], [\"quality\"], 3) or \\\n",
    "       check_proximity(text, term_lists[\"CollateralTerms\"], [\"valuation\"], 5) or \\\n",
    "       check_proximity(text, [\"haircut\"], term_lists[\"CollateralTerms\"], 3) or \\\n",
    "       check_proximity(text, [\"insufficient\"], term_lists[\"CollateralTerms\"], 3):\n",
    "        return \"Risk_CollateralQuality\"\n",
    "        \n",
    "    # 5. Risk_ResidualCredit\n",
    "    if re.search(r'residual risk|risk mitigation techniques', text, re.IGNORECASE) or \\\n",
    "       check_proximity(text, [\"residual\"], [\"credit risk\"], 3) or \\\n",
    "       check_proximity(text, [\"mitigant\", \"mitigants\"], [\"ineffective\"], 5):\n",
    "        return \"Risk_ResidualCredit\"\n",
    "\n",
    "    # 6. Risk_CountryTransfer\n",
    "    if check_list(text, term_lists[\"CountryTerms\"]) or \\\n",
    "       check_proximity(text, [\"restrictions\"], [\"capital transfer\"], 5) or \\\n",
    "       check_proximity(text, [\"sovereign\"], [\"default\"], 3) or \\\n",
    "       check_proximity(text, [\"country\"], [\"default risk\"], 3):\n",
    "        return \"Risk_CountryTransfer\"\n",
    "\n",
    "    # 7. Risk_MarketValue\n",
    "    if check_list(text, term_lists[\"MarketTerms\"]) or \\\n",
    "       check_proximity(text, [\"market\"], [\"risk\"], 3) or \\\n",
    "       check_proximity(text, [\"price\"], [\"volatility\"], 3) or \\\n",
    "       check_proximity(text, [\"trading book\"], [\"risk\"], 5):\n",
    "        return \"Risk_MarketValue\"\n",
    "\n",
    "    # 8. Risk_InterestRate_BankingBook\n",
    "    if check_list(text, term_lists[\"InterestRateTerms\"]) or \\\n",
    "       check_proximity(text, [\"interest rate\"], [\"banking book\"], 3) or \\\n",
    "       check_proximity(text, [\"repricing\"], [\"gap\"], 3) or \\\n",
    "       check_proximity(text, [\"earnings\"], [\"interest rate risk\"], 5):\n",
    "        return \"Risk_InterestRate_BankingBook\"\n",
    "\n",
    "    # 9. Risk_FXExposure\n",
    "    if check_list(text, term_lists[\"FXTerms\"]) or \\\n",
    "       check_proximity(text, [\"currency\"], [\"mismatch\", \"exposure\"], 3) or \\\n",
    "       check_proximity(text, [\"open FX\"], [\"position\"], 3) or \\\n",
    "       check_proximity(text, [\"foreign currency\"], [\"exposure\"], 3):\n",
    "        return \"Risk_FXExposure\"\n",
    "\n",
    "    # 10. Risk_CommodityPrice\n",
    "    if check_list(text, term_lists[\"CommodityTerms\"]) or \\\n",
    "       check_proximity(text, [\"commodity\"], [\"price risk\"], 3) or \\\n",
    "       check_proximity(text, [\"exposure\"], term_lists[\"CommodityTerms\"], 5):\n",
    "        return \"Risk_CommodityPrice\"\n",
    "\n",
    "    # 11. Risk_FundingLiquidity\n",
    "    if check_list(text, term_lists[\"LiquidityTerms\"]) or \\\n",
    "       check_proximity(text, [\"funding\"], [\"liquidity\"], 3) or \\\n",
    "       check_proximity(text, [\"funding\"], [\"stress\"], 5) or \\\n",
    "       check_proximity(text, [\"inability\"], [\"refinance\"], 5):\n",
    "        return \"Risk_FundingLiquidity\"\n",
    "\n",
    "    # 12. Risk_MarketLiquidity\n",
    "    if re.search(r'wide bid-ask spread|wide bid ask spread', text, re.IGNORECASE) or \\\n",
    "       check_proximity(text, [\"market liquidity\"], [\"risk\"], 3) or \\\n",
    "       check_proximity(text, [\"liquidity\"], [\"market depth\"], 5) or \\\n",
    "       check_proximity(text, [\"illiquid\"], [\"asset\", \"market\"], 3):\n",
    "        return \"Risk_MarketLiquidity\"\n",
    "\n",
    "    # 13. Risk_MaturityMismatch\n",
    "    if re.search(r'maturity mismatch|maturity transformation', text, re.IGNORECASE) or \\\n",
    "       check_proximity(text, [\"short-term funding\", \"short term funding\"], [\"long-term loan\", \"long term loan\"], 5):\n",
    "        return \"Risk_MaturityMismatch\"\n",
    "\n",
    "    # 14. Risk_DepositRunOff\n",
    "    if re.search(r'deposit run|bank run', text, re.IGNORECASE) or \\\n",
    "       check_proximity(text, [\"run\"], [\"deposits\"], 3) or \\\n",
    "       check_proximity(text, [\"run\"], [\"on the bank\"], 3) or \\\n",
    "       check_proximity(text, [\"outflow\"], [\"deposits\"], 3) or \\\n",
    "       check_proximity(text, [\"run-off\", \"run off\"], [\"rates\"], 3):\n",
    "        return \"Risk_DepositRunOff\"\n",
    "\n",
    "    # 15. Risk_OperationalProcess\n",
    "    if check_list(text, term_lists[\"OperationalTerms\"]) or \\\n",
    "       check_proximity(text, [\"process\"], [\"failure\", \"breakdown\", \"deficiency\"], 5) or \\\n",
    "       check_proximity(text, [\"internal control\"], [\"deficiency\"], 3) or \\\n",
    "       check_proximity(text, [\"manual process\"], [\"error\"], 5):\n",
    "        return \"Risk_OperationalProcess\"\n",
    "\n",
    "    # 16. Risk_ITSystemFailure\n",
    "    if check_list(text, term_lists[\"ITTerms\"]) or \\\n",
    "       check_proximity(text, [\"system\"], [\"outage\", \"failure\", \"crash\"], 3) or \\\n",
    "       check_proximity(text, [\"IT\"], [\"incident\", \"disruption\"], 3) or \\\n",
    "       check_proximity(text, [\"core banking system\"], [\"failure\"], 3):\n",
    "        return \"Risk_ITSystemFailure\"\n",
    "\n",
    "    # 17. Risk_CyberSecurity\n",
    "    if check_list(text, term_lists[\"CyberTerms\"]) or \\\n",
    "       check_proximity(text, [\"information security\"], [\"breach\"], 3) or \\\n",
    "       check_proximity(text, [\"unauthorized access\"], [\"systems\"], 3) or \\\n",
    "       check_proximity(text, [\"IT\"], [\"cyber attack\"], 5):\n",
    "        return \"Risk_CyberSecurity\"\n",
    "\n",
    "    # 18. Risk_ModelPerformance\n",
    "    if check_list(text, term_lists[\"ModelTerms\"]) or \\\n",
    "       re.search(r'model risk', text, re.IGNORECASE) or \\\n",
    "       check_proximity(text, [\"model\"], [\"mis-specification\", \"mis specification\", \"overfitting\"], 5) or \\\n",
    "       check_proximity(text, [\"model\"], [\"validation\", \"backtesting\", \"back testing\"], 5):\n",
    "        return \"Risk_ModelPerformance\"\n",
    "\n",
    "    # 19. Risk_DataQuality\n",
    "    if check_list(text, term_lists[\"DataQualityTerms\"]) or \\\n",
    "       check_proximity(text, [\"data\"], [\"error\", \"errors\", \"inaccuracy\", \"inaccuracies\"], 3) or \\\n",
    "       check_proximity(text, [\"incorrect\"], [\"data\"], 3) or \\\n",
    "       check_proximity(text, [\"missing\"], [\"data\"], 3):\n",
    "        return \"Risk_DataQuality\"\n",
    "\n",
    "    # 20. Risk_ThirdPartyOutsourcing\n",
    "    if check_list(text, term_lists[\"OutsourcingTerms\"]) or \\\n",
    "       check_proximity(text, [\"critical function\"], term_lists[\"OutsourcingTerms\"], 5) or \\\n",
    "       check_proximity(text, [\"outsourced\"], [\"service\", \"activity\"], 3) or \\\n",
    "       check_proximity(text, [\"third-party\", \"third party\"], [\"dependency\"], 5):\n",
    "        return \"Risk_ThirdPartyOutsourcing\"\n",
    "\n",
    "    # 21. Risk_LegalEnforceability\n",
    "    if check_list(text, term_lists[\"LegalTerms\"]) or \\\n",
    "       check_proximity(text, [\"legal\"], [\"enforceability\"], 3) or \\\n",
    "       check_proximity(text, [\"contract\"], [\"invalid\", \"unenforceable\"], 3) or \\\n",
    "       check_proximity(text, [\"legal\"], [\"uncertainty\"], 5):\n",
    "        return \"Risk_LegalEnforceability\"\n",
    "\n",
    "    # 22. Risk_RegulatoryCompliance\n",
    "    if check_list(text, term_lists[\"ComplianceTerms\"]) or \\\n",
    "       re.search(r'non-compliance|non compliance', text, re.IGNORECASE) or \\\n",
    "       check_proximity(text, [\"breach\"], [\"regulation\"], 5) or \\\n",
    "       check_proximity(text, [\"failure\"], [\"comply\", \"requirements\"], 5): # Simplified \"failure w/5 comply w/5 requirements\"\n",
    "        return \"Risk_RegulatoryCompliance\"\n",
    "\n",
    "    # 23. Risk_AML_CFT_Sanctions\n",
    "    if check_list(text, term_lists[\"AMLTerms\"]) or \\\n",
    "       check_proximity(text, [\"risk\"], [\"money laundering\"], 5) or \\\n",
    "       check_proximity(text, [\"risk\"], [\"terrorist financing\"], 5) or \\\n",
    "       check_proximity(text, [\"breach\"], [\"sanctions\"], 5):\n",
    "        return \"Risk_AML_CFT_Sanctions\"\n",
    "\n",
    "    # 24. Risk_ConsumerConduct\n",
    "    if check_list(text, term_lists[\"ConductTerms\"]) or \\\n",
    "       re.search(r'treating customers fairly', text, re.IGNORECASE) or \\\n",
    "       check_proximity(text, [\"consumer\"], [\"protection\", \"detriment\"], 3) or \\\n",
    "       check_proximity(text, [\"product\"], [\"unsuitable\"], 5):\n",
    "        return \"Risk_ConsumerConduct\"\n",
    "\n",
    "    # 25. Risk_ReputationImpact\n",
    "    if check_list(text, term_lists[\"ReputationTerms\"]) or \\\n",
    "       check_proximity(text, [\"reputation\"], [\"damage\"], 3) or \\\n",
    "       check_proximity(text, [\"adverse\"], [\"publicity\"], 3) or \\\n",
    "       check_proximity(text, [\"media\"], [\"criticism\", \"coverage\", \"scandal\"], 5):\n",
    "        return \"Risk_ReputationImpact\"\n",
    "\n",
    "    # 26. Risk_GovernanceControl\n",
    "    if check_list(text, term_lists[\"GovernanceTerms\"]) or \\\n",
    "       re.search(r'conflict of interest', text, re.IGNORECASE) or \\\n",
    "       check_proximity(text, [\"weak\"], [\"governance\"], 3) or \\\n",
    "       check_proximity(text, [\"inadequate\"], [\"internal control\"], 3) or \\\n",
    "       check_proximity(text, [\"lack of\"], [\"oversight\"], 5):\n",
    "        return \"Risk_GovernanceControl\"\n",
    "\n",
    "    # 27. Risk_StrategicBusiness\n",
    "    if check_list(text, term_lists[\"StrategicTerms\"]) or \\\n",
    "       check_proximity(text, [\"risk\"], [\"business model\"], 5) or \\\n",
    "       check_proximity(text, [\"strategy\"], [\"risk\"], 5) or \\\n",
    "       check_proximity(text, [\"strategic\"], [\"decision\"], 3):\n",
    "        return \"Risk_StrategicBusiness\"\n",
    "\n",
    "    # 28. Risk_SystemicInterconnected\n",
    "    if check_list(text, term_lists[\"SystemicTerms\"]) or \\\n",
    "       re.search(r'domino effect', text, re.IGNORECASE) or \\\n",
    "       check_proximity(text, [\"interconnectedness\"], [\"risk\"], 5) or \\\n",
    "       check_proximity(text, [\"failure\"], [\"financial system\"], 5):\n",
    "        return \"Risk_SystemicInterconnected\"\n",
    "\n",
    "    # 29. Risk_ClimateEnvironmental\n",
    "    if check_list(text, term_lists[\"ClimateTerms\"]) or \\\n",
    "       check_proximity(text, [\"physical\"], [\"climate risk\"], 3) or \\\n",
    "       check_proximity(text, [\"transition\"], [\"risk\"], 3) or \\\n",
    "       check_proximity(text, [\"climate-related\"], [\"risk\"], 3) or \\\n",
    "       check_proximity(text, [\"environmental\"], [\"risk\"], 3):\n",
    "        return \"Risk_ClimateEnvironmental\"\n",
    "\n",
    "    # 30. Risk_MacroeconomicCycle\n",
    "    if check_list(text, term_lists[\"MacroTerms\"]) or \\\n",
    "       re.search(r'economic downturn', text, re.IGNORECASE) or \\\n",
    "       check_proximity(text, [\"recession\"], [\"risk\"], 3) or \\\n",
    "       check_proximity(text, [\"macro\"], [\"shock\"], 3) or \\\n",
    "       check_proximity(text, [\"business cycle\"], [\"downturn\"], 3):\n",
    "        return \"Risk_MacroeconomicCycle\"\n",
    "\n",
    "\n",
    "    return \"Unclassified\"\n",
    "\n",
    "# Apply the Classifier to your DataFrame\n",
    "\n",
    "# THIS ASSUMES you have the 'clean_df' from the previous steps.\n",
    "# We will use the *original* 'Paragraph' text, as the rules need\n",
    "# full sentences, not the 'cleaned_paragraph'.\n",
    "\n",
    "# Make sure you have the 'clean_df' from Phase 1 first\n",
    "\n",
    "if 'clean_df' in locals():\n",
    "    print(\"\\n--- Phase 2: Starting Rules-Based Classification ---\")\n",
    "    start_classify = time.time()\n",
    "\n",
    "    # Apply the function to the *original* paragraph text\n",
    "    clean_df['classified_class'] = clean_df['Paragraph'].progress_apply(classify_risk_concept)\n",
    "\n",
    "    end_classify = time.time()\n",
    "    print(f\"Classification complete in {end_classify - start_classify:.2f} seconds.\")\n",
    "\n",
    "    # See Results\n",
    "    print(\"\\n--- Classification Results (Top 30) ---\")\n",
    "    print(clean_df['classified_class'].value_counts().head(30))\n",
    "\n",
    "    print(\"\\n--- Example of Classified Paragraphs ---\")\n",
    "    print(clean_df[clean_df['classified_class'] != 'Unclassified']\n",
    "          [['Paragraph', 'classified_class']].head())\n",
    "          \n",
    "    # Save the final file\n",
    "    print(\"\\nSaving final classified data...\")\n",
    "    final_save_path = '../classified_data/final_classified_paragraphs_3.csv'\n",
    "    # Make sure directory exists, etc.\n",
    "    clean_df.to_csv(final_save_path, index=False, sep='|')\n",
    "    print(f\"Final data saved to {final_save_path}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nError: 'clean_df' not found. Please run the Phase 1 filtering code first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f028c25",
   "metadata": {},
   "source": [
    "### Remove unclassified paragraphs and save new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149841b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Unclassified rows and short paragraphs\n",
    "new_df = clean_df[clean_df['classified_class'] != 'Unclassified']\n",
    "new_df = new_df[new_df[\"Paragraph\"].str.split().str.len() >= 20]\n",
    "new_df[\"Paragraph\"] = new_df[\"Paragraph\"].str.replace(r'^\\s*(?:\\(\\d+\\)|\\d+\\.)\\s*', '', regex=True)\n",
    "new_df.to_csv('classified_data_no_unclassified_3.csv', index=False, sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9ca367",
   "metadata": {},
   "source": [
    "### Second round of clustering to analyze missed keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810e946b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gap Analysis on \"Unclassified\" Data \n",
    "\n",
    "print(\"\\nAnalyzing Gaps in Classification\")\n",
    "\n",
    "# First, check if the previous steps ran and 'clean_df' exists\n",
    "if 'clean_df' not in locals():\n",
    "    print(\"Error: 'clean_df' not found. Please run the full script from Phase 1 and 2.\")\n",
    "else:\n",
    "    # 1. Isolate Unclassified Data\n",
    "    unclassified_df = clean_df[clean_df['classified_class'] == 'Unclassified'].copy()\n",
    "\n",
    "    unclassified_count = len(unclassified_df)\n",
    "    \n",
    "    if unclassified_count == 0:\n",
    "        print(\"ðŸŽ‰ Congratulations! No 'Unclassified' paragraphs were found.\")\n",
    "    else:\n",
    "        print(f\"Found {unclassified_count} 'Unclassified' paragraphs. Running clustering to analyze them...\")\n",
    "\n",
    "    # Set a K value for discovery.\n",
    "    k_discovery = 25 \n",
    "\n",
    "    # Check if we have enough data to cluster\n",
    "    if unclassified_count < k_discovery:\n",
    "        print(f\"Only {unclassified_count} unclassified items. Not enough to cluster into {k_discovery} groups.\")\n",
    "        if unclassified_count > 0:\n",
    "            print(\"Here are the unclassified paragraphs:\")\n",
    "            print(unclassified_df[['Paragraph_ID', 'Paragraph']].head())\n",
    "    \n",
    "    else:\n",
    "        # Vectorize *only* the unclassified text\n",
    "        print(f\"Vectorizing {unclassified_count} paragraphs...\")\n",
    "        start_vec_uc = time.time()\n",
    "        \n",
    "        # We use 'cleaned_paragraph' for better topic clustering\n",
    "        uc_vectorizer = TfidfVectorizer(max_df=0.90, \n",
    "                                        min_df=5,    # Ignore words that appear in < 5 docs\n",
    "                                        stop_words='english', \n",
    "                                        max_features=1000)\n",
    "        \n",
    "        uc_tfidf_matrix = uc_vectorizer.fit_transform(unclassified_df['cleaned_paragraph'])\n",
    "        \n",
    "        # Check if vectorization returned anything (min_df might filter everything)\n",
    "        if uc_tfidf_matrix.shape[0] == 0:\n",
    "            print(\"Error: No data remains after vectorization. Your 'min_df=5' might be too high for this small dataset.\")\n",
    "        \n",
    "        else:\n",
    "            end_vec_uc = time.time()\n",
    "            print(f\"Vectorization complete in {end_vec_uc - start_vec_uc:.2f}s. Matrix shape: {uc_tfidf_matrix.shape}\")\n",
    "\n",
    "            # 4. Run K-Means Clustering\n",
    "            print(f\"Running K-Means with k={k_discovery}...\")\n",
    "            start_kmeans_uc = time.time()\n",
    "\n",
    "            uc_kmeans = KMeans(n_clusters=k_discovery, random_state=42, n_init=10)\n",
    "            uc_kmeans.fit(uc_tfidf_matrix)\n",
    "            \n",
    "            # Add the new cluster IDs to the unclassified_df\n",
    "            unclassified_df['discovery_cluster'] = uc_kmeans.labels_\n",
    "            end_kmeans_uc = time.time()\n",
    "            print(f\"Clustering complete in {end_kmeans_uc - start_kmeans_uc:.2f}s.\")\n",
    "            print(f\"Inertia (lower is better): {uc_kmeans.inertia_}\")\n",
    "\n",
    "            # Analyze and Report the New Clusters\n",
    "            print(\"\\n--- ðŸ”Ž Top Words for Unclassified Topics ---\")\n",
    "            print(\"Review these topics to find gaps in your classification rules.\")\n",
    "            \n",
    "            uc_terms = uc_vectorizer.get_feature_names_out()\n",
    "            uc_order_centroids = uc_kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "            for i in range(k_discovery):\n",
    "                # Find the top words for this cluster\n",
    "                top_words = [uc_terms[ind] for ind in uc_order_centroids[i, :10]]\n",
    "                \n",
    "                # Get the count of paragraphs in this cluster\n",
    "                cluster_size = (unclassified_df['discovery_cluster'] == i).sum()\n",
    "                \n",
    "                print(f\"\\nDiscovery Cluster {i} (Size: {cluster_size}):\")\n",
    "                print(f\"  > {', '.join(top_words)}\")\n",
    "\n",
    "            print(\"\\nAnalysis of unclassified data is complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62713ddc",
   "metadata": {},
   "source": [
    "### BERT to analyze overlaps within classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26681ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Using device: cpu\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'final_combined_classified_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# ---------- LOAD DATA ----------\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m|\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m df[text_column] = df[text_column].astype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Drop empty cleaned paragraphs\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'final_combined_classified_data.csv'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# WARNING: You have to have torch and transformers installed\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "csv_path = \"\"\n",
    "text_column = \"cleaned_paragraph\"   # use only cleaned text\n",
    "class_column = \"classified_class\"   # filter within this class\n",
    "\n",
    "model_name = \"nlpaueb/legal-bert-base-uncased\"\n",
    "batch_size = 64\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ---------- LOAD DATA ----------\n",
    "df = pd.read_csv(csv_path, delimiter=\"|\")\n",
    "df[text_column] = df[text_column].astype(str)\n",
    "\n",
    "# Drop empty cleaned paragraphs\n",
    "df = df[df[text_column].str.strip() != \"\"].reset_index(drop=True)\n",
    "\n",
    "print(f\"Loaded {len(df)} rows with cleaned paragraphs.\")\n",
    "\n",
    "# ---------- LOAD MODEL ----------\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(device)\n",
    "model.eval()\n",
    "\n",
    "# ---------- EMBEDDING FUNCTION ----------\n",
    "def embed_texts(text_list, batch_size=16):\n",
    "    all_embs = []\n",
    "\n",
    "    for i in tqdm(range(0, len(text_list), batch_size), desc=\"Embedding\"):\n",
    "        batch_texts = text_list[i:i+batch_size]\n",
    "        enc = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**enc)\n",
    "            batch_embs = outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "        all_embs.append(batch_embs.cpu())\n",
    "\n",
    "    return torch.cat(all_embs, dim=0)\n",
    "\n",
    "# ---------- PROCESS EACH CLASS SEPARATELY ----------\n",
    "classes = df[class_column].unique()\n",
    "print(\"Classes found:\", classes)\n",
    "\n",
    "for cls in classes:\n",
    "    print(f\"\\n=== Processing class: {cls} ===\")\n",
    "    \n",
    "    subset = df[df[class_column] == cls].reset_index(drop=True)\n",
    "    texts = subset[text_column].astype(str).tolist()\n",
    "    \n",
    "    print(f\"{len(texts)} items in this class.\")\n",
    "\n",
    "    # ---- Create embeddings for this class only ----\n",
    "    embeddings = embed_texts(texts, batch_size=batch_size)\n",
    "    print(f\"Embeddings shape for class {cls}: {embeddings.shape}\")\n",
    "\n",
    "    # ---- Cosine similarity matrix ----\n",
    "    embeddings_norm = F.normalize(embeddings, p=2, dim=1)\n",
    "    similarity_matrix = embeddings_norm @ embeddings_norm.T\n",
    "\n",
    "    print(f\"Similarity matrix shape for class {cls}: {similarity_matrix.shape}\")\n",
    "\n",
    "    # ---- Save ----\n",
    "    filename = f\"similarity_{cls}.npy\"\n",
    "    np.save(filename, similarity_matrix.numpy())\n",
    "    print(f\"Saved:\", filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "945e4a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classified_class\n",
       "Risk_GovernanceControl           9681\n",
       "Risk_StrategicBusiness           7853\n",
       "Risk_MarketValue                 6673\n",
       "Risk_RegulatoryCompliance        5105\n",
       "Risk_CountryTransfer             4334\n",
       "Risk_AML_CFT_Sanctions           4048\n",
       "Risk_ThirdPartyOutsourcing       3368\n",
       "Risk_ClimateEnvironmental        3332\n",
       "Risk_FundingLiquidity            3278\n",
       "Risk_ModelPerformance            2498\n",
       "Risk_OperationalProcess          2043\n",
       "Risk_MacroeconomicCycle          1854\n",
       "Risk_SystemicInterconnected      1762\n",
       "Risk_CyberSecurity               1585\n",
       "Risk_CreditDefault               1557\n",
       "Risk_ConsumerConduct             1281\n",
       "Risk_LegalEnforceability         1268\n",
       "Risk_FXExposure                  1128\n",
       "Risk_DataQuality                  811\n",
       "Risk_CounterpartyCredit           564\n",
       "Risk_InterestRate_BankingBook     490\n",
       "Risk_ITSystemFailure              410\n",
       "Risk_Concentration                382\n",
       "Risk_CollateralQuality            310\n",
       "Risk_CommodityPrice               219\n",
       "Risk_ResidualCredit               212\n",
       "Risk_ReputationImpact              84\n",
       "Risk_MaturityMismatch              60\n",
       "Risk_DepositRunOff                 31\n",
       "Risk_MarketLiquidity                6\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"C:/Users/danit/Junction/final_data/final_combined_classified_data.csv\", delimiter=\"|\")\n",
    "df[\"classified_class\"].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
