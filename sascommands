/*===========================================================================*
* Flow:         mainFlow - Copy.flw
* ID:           a66fba21-c5dd-4ed3-9f48-19ae618ccf72
* Created:      2025-11-16T04:43:47.691503Z
* Created by:   gnudgnaoh.work@gmail.com
* Modified:     2025-11-16T05:37:17.061873Z
* Modified by:  gnudgnaoh.work@gmail.com
*
* Input Tables:
*   PUBLIC.TEXT_FOR_SAS_PARA_3
*
* Output Tables:
*   PUBLIC.CLEANED_DATA_PARA
*   PUBLIC.CLEANED_DATA_PARA
*   PUBLIC.CLEANED_DATA_PARA
*   PUBLIC.NEW_NEW_TEST_OUTPUT
*   PUBLIC.OVERLAPS
*   PUBLIC.CONTRADICTIONS
*
* Version:       DataFlows stable 2025.10 (20251114.1763153629253)
*
* Generated On:  2025-11-16T07:45:54.666084174Z
* Generated by:  gnudgnaoh.work@gmail.com
*============================================================================*/

/* region: Generated flow setup */
%let flow_id = %nrquote(%nrstr(a66fba21-c5dd-4ed3-9f48-19ae618ccf72));
%let flow_name = %nrquote(%nrstr(mainFlow - Copy.flw));
%let flow_place = %nrquote(%nrstr(SAS Content));
%let flow_location = %nrquote(%nrstr(/Public));
%macro _flw_action_start(nodes);
    data _null_;
        dtEndStr = put(datetime(), E8601DZ.);
        put "_FLW_ACTION_START_|" dtEndStr +(-1) "|&nodes";
    run;
%mend _flw_action_start;
%macro _flw_action_end(nodes, table_libs, libs, table_names);
    data _null_;

        attrib next_table_name length = $32 informat = $32. format = $32.
               dtStartStr length = $26 informat = $26. format = $26.;

        %local i next_table;
        %do i=1 %to %sysfunc(countc(&table_libs, |)) + 1;
            %let next_table = %qscan(&table_libs, &i, |);
            %let next_lib = %qscan(&libs, &i, |);
            %let next_table_name = %qscan(&table_names, &i, |);
            next_table_name = kreverse(ksubstr(kreverse(ksubstr(kstrip("&next_table_name."),2)),2));
            %let table_exists = %eval(%sysfunc(exist(&next_table, data)) or %sysfunc(exist(&next_table, view)));
            put "_FLW_ACTION_TABLE_|&next_lib|" next_table_name +(-1) "|&table_exists";
        %end;
        dtStartStr = put(datetime(), E8601DZ.);
        put "_FLW_ACTION_END_|" dtStartStr +(-1) "|&nodes";
    run;
%mend _flw_action_end;

/* endregion */

/*===========================================================================*
* Node name:        Data Cleaning & Clustering
* Node ID:          c794fde0-c283-11f0-93ed-a39222d825d4
*
* Input Tables:
*   PUBLIC.TEXT_FOR_SAS_PARA_3
*
* Output Tables:
*   PUBLIC.CLEANED_DATA_PARA
*
* Step name:        Python Program
* Step path:        /dataFlows/steps/ab59f8c4-af9a-4608-a5d5-a8365357bb99
* Step description: Run user written Python code.
*----------------------------------------------------------------------------*/

/* region: Generated step setup */

%_flw_action_start(c794fde0-c283-11f0-93ed-a39222d825d4);
/* endregion */


proc python;
submit;
_inputCount = 1
_outputCount = 1
_input1 = "PUBLIC.TEXT_FOR_SAS_PARA_3"
_output1 = "PUBLIC.CLEANED_DATA_PARA"
import pandas as pd
import re
# NLTK imports are removed
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
import time
from tqdm import tqdm

# --- Initialize tqdm for pandas ---
# This allows us to use .progress_apply() for a progress bar
tqdm.pandas(desc="Cleaning Text")

# --- NLTK downloads are removed ---

# --- Step 1: Pre-processing (Load & Clean) ---

print("--- Step 1: Loading & Cleaning ---")
start_load = time.time()

try:
    df = SAS.sd2df(_input1)
    end_load = time.time()
    print(f"Loaded {len(df)} rows in {end_load - start_load:.2f} seconds.")
except FileNotFoundError:
    print(f"Error: File not found at {file_path}")
    # Adding dummy data for demonstration

# --- NLTK tools (stopwords, lemmatizer) are removed ---

def preprocess_text(text):
    """Cleans text: lowercases, removes punctuation and numbers."""
    if not isinstance(text, str):
        return ""
    # Lowercase
    text = text.lower()
    # Remove punctuation
    text = re.sub(r'[^\w\s]', '', text)
    # Remove numbers
    text = re.sub(r'\d+', '', text)
    # NLTK tokenization, stop-word removal, and lemmatization are removed.
    # The vectorizer will handle stop words.
    return text  # Return the full cleaned string

print("Starting text preprocessing (this may take a while)...")
start_clean = time.time()
# Use .progress_apply() to see a progress bar
df['cleaned_paragraph'] = df['Paragraph'].progress_apply(preprocess_text)
end_clean = time.time()
print(f"Preprocessing complete in {end_clean - start_clean:.2f} seconds.")
print(df[['Paragraph', 'cleaned_paragraph']].head())

# --- Step 2: Vectorization (TF-IDF) ---
print("\n--- Step 2: Vectorization (TF-IDF) ---")
start_vec = time.time()
# The vectorizer will now handle stop words since NLTK is not.
vectorizer = TfidfVectorizer(max_df=0.90, min_df=5, stop_words='english', max_features=1000)

# Filter out empty paragraphs that might result from cleaning
original_rows = len(df)
non_empty_df = df[df['cleaned_paragraph'].str.strip().astype(bool)].copy()
rows_dropped = original_rows - len(non_empty_df)

if rows_dropped > 0:
    print(f"Filtered {rows_dropped} empty/unusable rows.")

if non_empty_df.empty:
    print("Error: No data left after cleaning or all paragraphs were shorter than min_df.")
else:
    print(f"Vectorizing {len(non_empty_df)} paragraphs...")
    tfidf_matrix = vectorizer.fit_transform(non_empty_df['cleaned_paragraph'])
    end_vec = time.time()
    print(f"Vectorization complete in {end_vec - start_vec:.2f} seconds.")
    print(f"TF-IDF Matrix Shape: (paragraphs, features) = {tfidf_matrix.shape}")

    # --- Step 3: Exploratory Clustering (K-Means) ---
    print("\n--- Step 3: Exploratory Clustering (K-Means) ---")
    start_cluster = time.time()
    
    # We pick a high-ish 'k' to find junk topics
    k_exploratory = 25  # <-- TUNE THIS NUMBER (try 30, 50, 75)
    
    # Ensure k is not larger than the number of samples
    if k_exploratory > tfidf_matrix.shape[0]:
        k_exploratory = tfidf_matrix.shape[0]
        print(f"Warning: k was larger than sample size. Setting k to {k_exploratory}.")

    print(f"Starting K-Means clustering with k={k_exploratory}...")
    kmeans = KMeans(n_clusters=k_exploratory, random_state=42, n_init=10)
    kmeans.fit(tfidf_matrix)
    end_cluster = time.time()
    
    # Add cluster labels back to our non-empty DataFrame
    non_empty_df['cluster_id'] = kmeans.labels_
    print(f"Clustering complete in {end_cluster - start_cluster:.2f} seconds.")
    print(f"Model Inertia (lower is better): {kmeans.inertia_}")


    # --- Step 4: Manual Inspection (Review Keywords) ---
    print("\n--- Step 4: Manual Inspection ---")
    print("\n--- Review Top Words per Cluster ---")
    print("Look for clusters with keywords like 'fishing', 'boat', 'court', etc.")
    
    terms = vectorizer.get_feature_names_out()
    order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]
    
    for i in range(k_exploratory):
        top_words = [terms[ind] for ind in order_centroids[i, :10]]
        print(f"Cluster {i}: {', '.join(top_words)}")

    # --- Step 5: Filtering & Saving Final DataFrame ---
    print("\n--- Step 5: Filtering & Saving ---")
    
    #
    # >>>>> CRITICAL HUMAN STEP <<<<<
    #
    # After reviewing the list above, make a list of the cluster IDs 
    # that represent junk topics (fishing, general legal, etc.).
    #
    # EXAMPLE: clusters_to_remove = [3, 12]
    #
    clusters_to_remove = [] # <-- POPULATE THIS MANUALLY
    
    #
    # >>>>> END HUMAN STEP <<<<<
    #
    
    if not clusters_to_remove:
        print("NOTE: `clusters_to_remove` is empty. No filtering will be applied.")
    
    # Filter the DataFrame to create the final "clean_df"
    clean_df = non_empty_df[~non_empty_df['cluster_id'].isin(clusters_to_remove)].copy()
    
    print("\nFiltering complete.")
    print(f"Original non-empty paragraphs: {len(non_empty_df)}")
    print(f"Paragraphs removed: {len(non_empty_df) - len(clean_df)}")
    print(f"Clean paragraphs remaining: {len(clean_df)}")
    
    # --- Save the Final Cleaned & Filtered DataFrame ---
    print("\nSaving final cleaned & filtered data...")
    save_path = '../filtered_data/filtered_paragraphs_final.csv'
    
    try:
        clean_df.to_csv(save_path, index=False, sep='|')
        print(f"Final cleaned data saved to {save_path}")
    except Exception as e:
        print(f"Error saving file: {e}. Make sure the directory '../filtered_data/' exists.")

ds = SAS.df2sd(clean_df, _output1)
    
endsubmit;
quit;

/* region: Generated step cleanup for Data Cleaning & Clustering */
%_flw_action_end(c794fde0-c283-11f0-93ed-a39222d825d4, PUBLIC.CLEANED_DATA_PARA, PUBLIC, "CLEANED_DATA_PARA");
/* endregion */

/*===========================================================================*
* Node name:        Cluster Filtering 
* Node ID:          68310f90-c285-11f0-93ed-a39222d825d4
*
* Input Tables:
*   PUBLIC.CLEANED_DATA_PARA
*
* Output Tables:
*   PUBLIC.CLEANED_DATA_PARA
*
* Step name:        Python Program
* Step path:        /dataFlows/steps/ab59f8c4-af9a-4608-a5d5-a8365357bb99
* Step description: Run user written Python code.
*----------------------------------------------------------------------------*/

/* region: Generated step setup */

%_flw_action_start(68310f90-c285-11f0-93ed-a39222d825d4);
/* endregion */


proc python;
submit;
_inputCount = 1
_outputCount = 1
_input1 = "PUBLIC.CLEANED_DATA_PARA"
_output1 = "PUBLIC.CLEANED_DATA_PARA"
import pandas as pd
import time

# --- Step 4.5: Load data from SAS ---
# This is the new starting point as requested.
# 'df' now holds the data from your SAS input dataset.
print("Loading SAS dataset _input1 into pandas DataFrame...")
df = SAS.sd2df(_input1)
print(f"Loaded {len(df)} rows from SAS.")

# --- Step 5: Filtering ---
print("\n--- Step 5: Filtering ---")

#
# >>>>> CRITICAL HUMAN STEP <<<<<
#
# After reviewing the list above, make a list of the cluster IDs 
# that represent junk topics (fishing, general legal, etc.).
#
# EXAMPLE: clusters_to_remove = [3, 12, 29, 45]
#
clusters_to_remove = [] # <-- POPULATE THIS MANUALLY

#
# >>>>> END HUMAN STEP <<<<<
#

if not clusters_to_remove:
    print("NOTE: `clusters_to_remove` is empty. No filtering will be applied.")
else:
    print(f"Filtering out cluster IDs: {clusters_to_remove}")

# Filter the DataFrame
# The logic is the same, but it operates on 'df' (from SAS)
# instead of 'non_empty_df'.
clean_df = df[~df['cluster_id'].isin(clusters_to_remove)].copy()

print("\nFiltering complete.")
print(f"Original non-empty paragraphs: {len(df)}")
print(f"Paragraphs removed: {len(df) - len(clean_df)}")
print(f"Clean paragraphs remaining: {len(clean_df)}")

# --- Step 6: Save Filtered Data back to SAS ---
# This replaces the .to_csv() step with the SAS equivalent
print("\nSaving filtered data back to SAS...")
start_save = time.time()

# Write the pandas DataFrame 'clean_df' to a new SAS dataset in the WORK library
# You can change 'filtered_paragraphs' to 'libref.dataset' if needed
sas_output_dataset = 'filtered_paragraphs'
SAS.df2sd(clean_df, sas_output_dataset) 

end_save = time.time()
print(f"Filtered data saved to SAS dataset '{sas_output_dataset}' in {end_save - start_save:.2f} seconds.")
    
endsubmit;
quit;

/* region: Generated step cleanup for Cluster Filtering */
%_flw_action_end(68310f90-c285-11f0-93ed-a39222d825d4, PUBLIC.CLEANED_DATA_PARA, PUBLIC, "CLEANED_DATA_PARA");
/* endregion */

/*===========================================================================*
* Node name:        Classifying Risk Concepts
* Node ID:          65d0bbf0-c286-11f0-93ed-a39222d825d4
*
* Input Tables:
*   PUBLIC.CLEANED_DATA_PARA
*
* Output Tables:
*   PUBLIC.CLEANED_DATA_PARA
*
* Step name:        Python Program
* Step path:        /dataFlows/steps/ab59f8c4-af9a-4608-a5d5-a8365357bb99
* Step description: Run user written Python code.
*----------------------------------------------------------------------------*/

/* region: Generated step setup */

%_flw_action_start(65d0bbf0-c286-11f0-93ed-a39222d825d4);
/* endregion */


proc python;
submit;
_inputCount = 1
_outputCount = 1
_input1 = "PUBLIC.CLEANED_DATA_PARA"
_output1 = "PUBLIC.CLEANED_DATA_PARA"
import re
import time
import pandas as pd
from tqdm import tqdm

# Ensure tqdm is set up for pandas
tqdm.pandas(desc="Classifying Risks")

clean_df = SAS.sd2df(_input1)

# --- Step 1: Define all Term Lists (Expanded Version) ---

# This dictionary holds all your keyword lists
term_lists = {
    "RiskTriggerTerms": ["risk", "risks", "exposure", "exposures", "vulnerability", "vulnerabilities", "hazard", "threat", "peril"],
    
    "DefaultTerms": ["default", "defaults", "non-performing", "non performing", "past due", "delinquency", "delinquent", 
                     "arrears", "insolvency", "bankruptcy", "foreclosure", "write-off", "charge-off", "impaired loan", "credit loss"],
                     
    "CreditTerms": ["credit", "loan", "loans", "lending", "obligor", "borrower", "debtor", "mortgage", "creditor", 
                    "debenture", "bond", "overdraft", "facility", "credit line", "loan portfolio", "credit institution"],
                    
    "CounterpartyTerms": ["counterparty", "counterparties", "trading partner", "derivative counterparty", "clearing member", 
                          "CCP", "central counterparty", "clearing house", "bilateral agreement", "ISDA"],
                          
    "CollateralTerms": ["collateral", "security", "securities", "pledge", "guarantee", "guarantor", "margin", 
                        "encumbrance", "lien", "charge", "hypothecation", "repo", "reverse repo", "security interest", "property"],
                        
    "CountryTerms": ["country risk", "transfer risk", "sovereign risk", "cross-border", "cross border", "geopolitical risk", "sovereign default"],
    
    "MarketTerms": ["market risk", "market volatility", "price volatility", "spread risk", "trading book", "equities", "bonds", 
                    "commodities", "derivatives", "spreads", "VaR", "value at risk", "expected shortfall", "ES", "mark-to-market", "MTM"],
                    
    "InterestRateTerms": ["interest rate risk", "IRRBB", "yield curve", "repricing gap", "net interest income", "NII", 
                          "economic value of equity", "EVE", "basis risk", "yield curve risk", "repricing risk"],
                          
    "LiquidityTerms": ["liquidity risk", "funding liquidity", "market liquidity", "liquidity coverage ratio", "LCR", "NSFR", 
                       "net stable funding ratio", "liquidity buffer", "cash flow", "funding mismatch", "run-off", "deposit outflow", 
                       "HQLA", "high-quality liquid assets", "ILAAP", "contingency funding plan", "CFP"],
                       
    "FXTerms": ["foreign exchange", "FX risk", "currency mismatch", "exchange rate", "currency risk", "FX exposure", 
                "translation risk", "transaction risk", "foreign currency risk"],
                
    "CommodityTerms": ["commodity price", "commodity risk", "oil price", "gas price", "energy price", "metal price", "agricultural price"],
    
    "OperationalTerms": ["operational risk", "operational failure", "internal process", "process failure", "error", "manual error", 
                         "oprisk", "RCSA", "risk and control self-assessment", "loss event", "failed process", "people risk", 
                         "business disruption", "BCP", "business continuity", "internal fraud", "external fraud"],
                         
    "ITTerms": ["IT system", "system outage", "system failure", "downtime", "IT disruption", "legacy system", "core banking system", 
                "infrastructure failure", "network outage", "disaster recovery", "DR", "technology failure", "platform instability"],
                
    "CyberTerms": ["cyber risk", "cybersecurity", "cyber attack", "malware", "ransomware", "data breach", "infosec", 
                   "information security", "phishing", "vishing", "smishing", "denial of service", "DoS", "DDoS", 
                   "zero-day", "exploit", "unauthorized access", "cyber threat", "cyber incident"],
                   
    "ModelTerms": ["risk model", "scoring model", "PD model", "LGD model", "EAD model", "model validation", "model risk", 
                   "model uncertainty", "model error", "model overlay", "back-testing", "backtesting", "model governance", 
                   "model implementation", "model misuse", "pd", "probability of default", "lgd", "loss given default", "estimation", "downturn"],
                   
    "DataQualityTerms": ["data quality", "data integrity", "data accuracy", "data completeness", "data inconsistency", 
                         "data governance", "data lineage", "data validation", "reconciliation", "inaccurate data", 
                         "incomplete data", "data stewardship", "data reliability"],
                         
    "OutsourcingTerms": ["outsourcing", "third-party", "third party", "external service provider", "cloud service provider", 
                         "fourth-party", "sub-contractor", "supply chain risk", "vendor risk", "service level agreement", "SLA", 
                         "vendor management", "supplier risk"],
                         
    "LegalTerms": ["legal risk", "enforceability", "legal enforceability", "litigation", "legal dispute", "non-enforceable", 
                   "unenforceable", "contract risk", "legal uncertainty", "regulatory action", "lawsuit", "arbitration"],
                   
    "ComplianceTerms": ["compliance risk", "regulatory requirements", "supervisory expectations", "prudential requirements", 
                        "regulatory reporting", "non-compliance", "breach", "regulatory breach", "supervisory action", 
                        "regulatory fine", "regulatory scrutiny", "supervisory review"],
                        
    "AMLTerms": ["money laundering", "terrorist financing", "AML", "CFT", "sanctions", "sanctioned entity", "KYC", 
                 "know your customer", "PEP", "politically exposed person", "SAR", "suspicious activity report", 
                 "OFAC", "financial crime", "fatf", "financial action task force", "mltf"],
                 
    "ConductTerms": ["conduct risk", "mis-selling", "mis selling", "consumer protection", "treating customers fairly", "TCF", 
                     "product suitability", "client detriment", "conflict of interest", "insider trading", "market abuse", "market manipulation"],
                     
    "ReputationTerms": ["reputational risk", "reputation risk", "adverse publicity", "public perception", "media coverage", 
                        "brand damage", "public trust", "scandal", "negative press", "stakeholder perception", "loss of trust"],
                        
    "GovernanceTerms": ["governance", "internal control", "risk committee", "board oversight", "risk culture", "conflict of interest", 
                        "risk appetite", "three lines of defense", "3LOD", "audit committee", "remuneration", "corporate governance", 
                        "board of directors", "management body", "accountability"],
                        
    "StrategicTerms": ["strategic risk", "business model", "strategy", "business strategy", "competitive risk", "business environment", 
                       "merger", "acquisition", "M&A", "business model viability", "competitive landscape"],
                       
    "SystemicTerms": ["systemic risk", "macroprudential", "contagion", "spillover", "too big to fail", "interconnectedness", 
                      "financial stability", "domino effect", "G-SIB", "G-SII", "globally systemically important"],
                      
    "ClimateTerms": ["climate risk", "environmental risk", "physical risk", "transition risk", "ESG risk", "greenhouse gas", 
                     "sustainability", "carbon footprint", "stranded assets", "TCFD", "greenwashing", "environmental social governance"],
                     
    "MacroTerms": ["macroeconomic", "business cycle", "economic downturn", "recession", "inflation shock", "geopolitical", 
                   "interest rate shock", "stagflation", "unemployment", "GDP growth", "economic shock", "inflationary pressure"],
                   
    "RequirementTerms": ["requirement", "requirements", "obligation", "obligations", "shall", "must", "binding", "mandatory"],
    
    "CapitalTerms": ["capital requirement", "capital buffer", "Pillar 2", "own funds", "capital adequacy", "CET1", "Common Equity Tier 1", 
                     "Tier 1 capital", "Tier 2 capital", "T1", "T2", "Basel III", "Basel 3", "Basel IV", "Basel 4", "ICAAP", 
                     "leverage ratio", "RWA", "risk-weighted assets", "capital ratio", "Pillar 1", "irb", "internal ratings based", "standardized approach", "securisation", "asset", "liability", "equity", "share", "tier"]
}

# --- Step 2: Helper Functions for Rule Logic ---

def build_regex_pattern(terms):
    """Builds a regex pattern from a list of terms."""
    # We sort by length, longest first, to match "non performing" before "non"
    sorted_terms = sorted(terms, key=len, reverse=True)
    # Create pattern: \b(term1|term2|...)\b
    return r'\b(' + '|'.join(re.escape(term) for term in sorted_terms) + r')\b'

def check_proximity(text, terms1_list, terms2_list, distance):
    """
    Checks if a term from terms1 is within 'distance' words of a term from terms2.
    """
    # Create regex patterns for both lists
    pattern1 = build_regex_pattern(terms1_list)
    pattern2 = build_regex_pattern(terms2_list)
    
    # Create forward and backward proximity patterns
    # (A w/N B) -> A ... B
    forward_pattern = pattern1 + (r'(?:\s+\w+){0,' + str(distance) + r'}\s+') + pattern2
    # (B w/N A) -> B ... A
    backward_pattern = pattern2 + (r'(?:\s+\w+){0,' + str(distance) + r'}\s+') + pattern1

    # Check if either pattern exists
    if re.search(forward_pattern, text, re.IGNORECASE) or \
       re.search(backward_pattern, text, re.IGNORECASE):
        return True
    return False

def check_list(text, terms_list):
    """Checks if any term from the list exists in the text."""
    pattern = build_regex_pattern(terms_list)
    if re.search(pattern, text, re.IGNORECASE):
        return True
    return False

# --- Step 3: The Main Classifier Function ---

def classify_risk_concept(text):
    """
    Classifies text based on the 30-rule concept model.
    Uses a 'first match wins' logic.
    """
    if not isinstance(text, str):
        return "Unclassified"
    
    # 1. Risk_CreditDefault
    if check_proximity(text, term_lists["CreditTerms"], term_lists["DefaultTerms"], 3) or \
       check_proximity(text, term_lists["DefaultTerms"], ["exposure"], 3) or \
       check_proximity(text, term_lists["RiskTriggerTerms"], term_lists["DefaultTerms"], 3) or \
       re.search(r'credit default risk|risk of default', text, re.IGNORECASE):
        return "Risk_CreditDefault"

    # 2. Risk_CounterpartyCredit
    if check_proximity(text, term_lists["CounterpartyTerms"], term_lists["DefaultTerms"], 5) or \
       check_proximity(text, term_lists["CounterpartyTerms"], ["credit risk"], 5) or \
       check_proximity(text, ["settlement"], term_lists["CounterpartyTerms"], 5) or \
       re.search(r'counterparty credit risk', text, re.IGNORECASE):
        return "Risk_CounterpartyCredit"
    
    # 3. Risk_Concentration
    if re.search(r'concentration risk|large exposure|large exposures', text, re.IGNORECASE) or \
       check_proximity(text, ["sectoral"], ["concentration"], 3) or \
       check_proximity(text, ["portfolio"], ["concentration"], 3):
        return "Risk_Concentration"

    # 4. Risk_CollateralQuality
    if check_proximity(text, term_lists["CollateralTerms"], ["quality"], 3) or \
       check_proximity(text, term_lists["CollateralTerms"], ["valuation"], 5) or \
       check_proximity(text, ["haircut"], term_lists["CollateralTerms"], 3) or \
       check_proximity(text, ["insufficient"], term_lists["CollateralTerms"], 3):
        return "Risk_CollateralQuality"
        
    # 5. Risk_ResidualCredit
    if re.search(r'residual risk|risk mitigation techniques', text, re.IGNORECASE) or \
       check_proximity(text, ["residual"], ["credit risk"], 3) or \
       check_proximity(text, ["mitigant", "mitigants"], ["ineffective"], 5):
        return "Risk_ResidualCredit"

    # 6. Risk_CountryTransfer
    if check_list(text, term_lists["CountryTerms"]) or \
       check_proximity(text, ["restrictions"], ["capital transfer"], 5) or \
       check_proximity(text, ["sovereign"], ["default"], 3) or \
       check_proximity(text, ["country"], ["default risk"], 3):
        return "Risk_CountryTransfer"

    # 7. Risk_MarketValue
    if check_list(text, term_lists["MarketTerms"]) or \
       check_proximity(text, ["market"], ["risk"], 3) or \
       check_proximity(text, ["price"], ["volatility"], 3) or \
       check_proximity(text, ["trading book"], ["risk"], 5):
        return "Risk_MarketValue"

    # 8. Risk_InterestRate_BankingBook
    if check_list(text, term_lists["InterestRateTerms"]) or \
       check_proximity(text, ["interest rate"], ["banking book"], 3) or \
       check_proximity(text, ["repricing"], ["gap"], 3) or \
       check_proximity(text, ["earnings"], ["interest rate risk"], 5):
        return "Risk_InterestRate_BankingBook"

    # 9. Risk_FXExposure
    if check_list(text, term_lists["FXTerms"]) or \
       check_proximity(text, ["currency"], ["mismatch", "exposure"], 3) or \
       check_proximity(text, ["open FX"], ["position"], 3) or \
       check_proximity(text, ["foreign currency"], ["exposure"], 3):
        return "Risk_FXExposure"

    # 10. Risk_CommodityPrice
    if check_list(text, term_lists["CommodityTerms"]) or \
       check_proximity(text, ["commodity"], ["price risk"], 3) or \
       check_proximity(text, ["exposure"], term_lists["CommodityTerms"], 5):
        return "Risk_CommodityPrice"

    # 11. Risk_FundingLiquidity
    if check_list(text, term_lists["LiquidityTerms"]) or \
       check_proximity(text, ["funding"], ["liquidity"], 3) or \
       check_proximity(text, ["funding"], ["stress"], 5) or \
       check_proximity(text, ["inability"], ["refinance"], 5):
        return "Risk_FundingLiquidity"

    # 12. Risk_MarketLiquidity
    if re.search(r'wide bid-ask spread|wide bid ask spread', text, re.IGNORECASE) or \
       check_proximity(text, ["market liquidity"], ["risk"], 3) or \
       check_proximity(text, ["liquidity"], ["market depth"], 5) or \
       check_proximity(text, ["illiquid"], ["asset", "market"], 3):
        return "Risk_MarketLiquidity"

    # 13. Risk_MaturityMismatch
    if re.search(r'maturity mismatch|maturity transformation', text, re.IGNORECASE) or \
       check_proximity(text, ["short-term funding", "short term funding"], ["long-term loan", "long term loan"], 5):
        return "Risk_MaturityMismatch"

    # 14. Risk_DepositRunOff
    if re.search(r'deposit run|bank run', text, re.IGNORECASE) or \
       check_proximity(text, ["run"], ["deposits"], 3) or \
       check_proximity(text, ["run"], ["on the bank"], 3) or \
       check_proximity(text, ["outflow"], ["deposits"], 3) or \
       check_proximity(text, ["run-off", "run off"], ["rates"], 3):
        return "Risk_DepositRunOff"

    # 15. Risk_OperationalProcess
    if check_list(text, term_lists["OperationalTerms"]) or \
       check_proximity(text, ["process"], ["failure", "breakdown", "deficiency"], 5) or \
       check_proximity(text, ["internal control"], ["deficiency"], 3) or \
       check_proximity(text, ["manual process"], ["error"], 5):
        return "Risk_OperationalProcess"

    # 16. Risk_ITSystemFailure
    if check_list(text, term_lists["ITTerms"]) or \
       check_proximity(text, ["system"], ["outage", "failure", "crash"], 3) or \
       check_proximity(text, ["IT"], ["incident", "disruption"], 3) or \
       check_proximity(text, ["core banking system"], ["failure"], 3):
        return "Risk_ITSystemFailure"

    # 17. Risk_CyberSecurity
    if check_list(text, term_lists["CyberTerms"]) or \
       check_proximity(text, ["information security"], ["breach"], 3) or \
       check_proximity(text, ["unauthorized access"], ["systems"], 3) or \
       check_proximity(text, ["IT"], ["cyber attack"], 5):
        return "Risk_CyberSecurity"

    # 18. Risk_ModelPerformance
    if check_list(text, term_lists["ModelTerms"]) or \
       re.search(r'model risk', text, re.IGNORECASE) or \
       check_proximity(text, ["model"], ["mis-specification", "mis specification", "overfitting"], 5) or \
       check_proximity(text, ["model"], ["validation", "backtesting", "back testing"], 5):
        return "Risk_ModelPerformance"

    # 19. Risk_DataQuality
    if check_list(text, term_lists["DataQualityTerms"]) or \
       check_proximity(text, ["data"], ["error", "errors", "inaccuracy", "inaccuracies"], 3) or \
       check_proximity(text, ["incorrect"], ["data"], 3) or \
       check_proximity(text, ["missing"], ["data"], 3):
        return "Risk_DataQuality"

    # 20. Risk_ThirdPartyOutsourcing
    if check_list(text, term_lists["OutsourcingTerms"]) or \
       check_proximity(text, ["critical function"], term_lists["OutsourcingTerms"], 5) or \
       check_proximity(text, ["outsourced"], ["service", "activity"], 3) or \
       check_proximity(text, ["third-party", "third party"], ["dependency"], 5):
        return "Risk_ThirdPartyOutsourcing"

    # 21. Risk_LegalEnforceability
    if check_list(text, term_lists["LegalTerms"]) or \
       check_proximity(text, ["legal"], ["enforceability"], 3) or \
       check_proximity(text, ["contract"], ["invalid", "unenforceable"], 3) or \
       check_proximity(text, ["legal"], ["uncertainty"], 5):
        return "Risk_LegalEnforceability"

    # 22. Risk_RegulatoryCompliance
    if check_list(text, term_lists["ComplianceTerms"]) or \
       re.search(r'non-compliance|non compliance', text, re.IGNORECASE) or \
       check_proximity(text, ["breach"], ["regulation"], 5) or \
       check_proximity(text, ["failure"], ["comply", "requirements"], 5): # Simplified "failure w/5 comply w/5 requirements"
        return "Risk_RegulatoryCompliance"

    # 23. Risk_AML_CFT_Sanctions
    if check_list(text, term_lists["AMLTerms"]) or \
       check_proximity(text, ["risk"], ["money laundering"], 5) or \
       check_proximity(text, ["risk"], ["terrorist financing"], 5) or \
       check_proximity(text, ["breach"], ["sanctions"], 5):
        return "Risk_AML_CFT_Sanctions"

    # 24. Risk_ConsumerConduct
    if check_list(text, term_lists["ConductTerms"]) or \
       re.search(r'treating customers fairly', text, re.IGNORECASE) or \
       check_proximity(text, ["consumer"], ["protection", "detriment"], 3) or \
       check_proximity(text, ["product"], ["unsuitable"], 5):
        return "Risk_ConsumerConduct"

    # 25. Risk_ReputationImpact
    if check_list(text, term_lists["ReputationTerms"]) or \
       check_proximity(text, ["reputation"], ["damage"], 3) or \
       check_proximity(text, ["adverse"], ["publicity"], 3) or \
       check_proximity(text, ["media"], ["criticism", "coverage", "scandal"], 5):
        return "Risk_ReputationImpact"

    # 26. Risk_GovernanceControl
    if check_list(text, term_lists["GovernanceTerms"]) or \
       re.search(r'conflict of interest', text, re.IGNORECASE) or \
       check_proximity(text, ["weak"], ["governance"], 3) or \
       check_proximity(text, ["inadequate"], ["internal control"], 3) or \
       check_proximity(text, ["lack of"], ["oversight"], 5):
        return "Risk_GovernanceControl"

    # 27. Risk_StrategicBusiness
    if check_list(text, term_lists["StrategicTerms"]) or \
       check_proximity(text, ["risk"], ["business model"], 5) or \
       check_proximity(text, ["strategy"], ["risk"], 5) or \
       check_proximity(text, ["strategic"], ["decision"], 3):
        return "Risk_StrategicBusiness"

    # 28. Risk_SystemicInterconnected
    if check_list(text, term_lists["SystemicTerms"]) or \
       re.search(r'domino effect', text, re.IGNORECASE) or \
       check_proximity(text, ["interconnectedness"], ["risk"], 5) or \
       check_proximity(text, ["failure"], ["financial system"], 5):
        return "Risk_SystemicInterconnected"

    # 29. Risk_ClimateEnvironmental
    if check_list(text, term_lists["ClimateTerms"]) or \
       check_proximity(text, ["physical"], ["climate risk"], 3) or \
       check_proximity(text, ["transition"], ["risk"], 3) or \
       check_proximity(text, ["climate-related"], ["risk"], 3) or \
       check_proximity(text, ["environmental"], ["risk"], 3):
        return "Risk_ClimateEnvironmental"

    # 30. Risk_MacroeconomicCycle
    if check_list(text, term_lists["MacroTerms"]) or \
       re.search(r'economic downturn', text, re.IGNORECASE) or \
       check_proximity(text, ["recession"], ["risk"], 3) or \
       check_proximity(text, ["macro"], ["shock"], 3) or \
       check_proximity(text, ["business cycle"], ["downturn"], 3):
        return "Risk_MacroeconomicCycle"


    return "Unclassified"

# --- Step 4: Apply the Classifier to your DataFrame ---

# THIS ASSUMES you have the 'clean_df' from the previous steps.
# We will use the *original* 'Paragraph' text, as the rules need
# full sentences, not the 'cleaned_paragraph'.

# Make sure you have the 'clean_df' from Phase 1 first
# Example: clean_df = pd.read_csv('../filtered_data/filtered_paragraphs.csv', sep='|') 

if 'clean_df' in locals():
    print("\n--- Phase 2: Starting Rules-Based Classification ---")
    start_classify = time.time()

    # Apply the function to the *original* paragraph text
    clean_df['classified_class'] = clean_df['Paragraph'].progress_apply(classify_risk_concept)

    end_classify = time.time()
    print(f"Classification complete in {end_classify - start_classify:.2f} seconds.")

    # --- See Your Results ---
    print("\n--- Classification Results (Top 30) ---")
    print(clean_df['classified_class'].value_counts().head(30))

    print("\n--- Example of Classified Paragraphs ---")
    print(clean_df[clean_df['classified_class'] != 'Unclassified']
          [['Paragraph', 'classified_class']].head())
          
    # Save the final file
    SAS.df2sd(clean_df, _output1)

else:
    print("\nError: 'clean_df' not found. Please run the Phase 1 filtering code first.")
    
endsubmit;
quit;

/* region: Generated step cleanup for Classifying Risk Concepts */
%_flw_action_end(65d0bbf0-c286-11f0-93ed-a39222d825d4, PUBLIC.CLEANED_DATA_PARA, PUBLIC, "CLEANED_DATA_PARA");
/* endregion */

/*===========================================================================*
* Node name:        Filter Unclassified
* Node ID:          e3a3e2d0-c2ab-11f0-808a-536665d79561
*
* Input Tables:
*   PUBLIC.CLEANED_DATA_PARA
*
* Output Tables:
*   WORK._flw000e3a3e2d0c2ab11f0_0_0_1
*
* Step name:        Filter Rows
* Step path:        /dataFlows/steps/29351cc4-ab8e-40ed-804b-109f0b5a9b93
* Step description: Filter rows
*----------------------------------------------------------------------------*/

/* region: Generated step setup */

%_flw_action_start(e3a3e2d0-c2ab-11f0-808a-536665d79561);
/* endregion */


/* Delete table or view WORK._flw000e3a3e2d0c2ab11f0_0_0_1 */
proc datasets library = WORK memtype = (data view) nolist nowarn;
   delete _flw000e3a3e2d0c2ab11f0_0_0_1;
quit;

data WORK._flw000e3a3e2d0c2ab11f0_0_0_1;
   set PUBLIC.CLEANED_DATA_PARA;
   where kupcase(classified_class) ne "UNCLASSIFIED";
run;


/* region: Generated step cleanup for Filter Unclassified */
%_flw_action_end(e3a3e2d0-c2ab-11f0-808a-536665d79561, WORK._flw000e3a3e2d0c2ab11f0_0_0_1, WORK, "_flw000e3a3e2d0c2ab11f0_0_0_1");
/* endregion */

/*===========================================================================*
* Node name:        LLM API Call
* Node ID:          071d40d0-c2a7-11f0-8cf1-97ecc97c9d4e
*
* Input Tables:
*   WORK._flw000e3a3e2d0c2ab11f0_0_0_1
*
* Output Tables:
*   PUBLIC.NEW_NEW_TEST_OUTPUT
*
* Step name:        Python Program
* Step path:        /dataFlows/steps/ab59f8c4-af9a-4608-a5d5-a8365357bb99
* Step description: Run user written Python code.
*----------------------------------------------------------------------------*/

/* region: Generated step setup */

%_flw_action_start(071d40d0-c2a7-11f0-8cf1-97ecc97c9d4e);
/* endregion */


proc python;
submit;
_inputCount = 1
_outputCount = 1
_input1 = "WORK._flw000e3a3e2d0c2ab11f0_0_0_1"
_output1 = "PUBLIC.NEW_NEW_TEST_OUTPUT"
import pandas as pd
import urllib.request
import json

df = SAS.sd2df(_input1)

# Make it small, only 30 paragraphs
df = df.head(30)

# Create pairwise combinations
df1 = df.copy()
df2 = df.copy()
pairs = pd.merge(df1, df2, how='cross', suffixes=('_1', '_2'))

# Filter to avoid duplicates and self-pairs (assuming unique pairs are desired)
pairs = pairs[pairs['Paragraph_ID_1'] < pairs['Paragraph_ID_2']]


# Assuming pairs is already defined as shown
# Replace with your actual API key
API_KEY = 'rc_2fa21ec6cdfc7c47812190285ebfc63c52f38e918e6181bd15a5020387f766c5'
BASE_URL = 'https://api.featherless.ai/v1'

def classify_relationships_batch(batch_df):
    # Build a single prompt with all pairs in the batch
    prompt_parts = []
    for i, row in batch_df.iterrows():
        prompt_parts.append(f"Pair {i+1}:\nText1: {row['Paragraph_1']}\nText2: {row['Paragraph_2']}\n")
    
    prompt = f"""
    Classify the relationship for each pair below into one of these categories:
    - 1 if the texts content overlap significantly with each other, you can remove one without any loss of data.
    - 2 if the texts directly oppose or contradict each other.
    - 0 if the texts have no meaningful connection or relation.
   
    {"".join(prompt_parts)}
   
    Respond with a JSON object in this format: {{"relationships": ["category_for_pair1", "category_for_pair2", ...]}}
    Include only the JSON, nothing else.
    """
    
    data = {
        "model": "deepseek-ai/DeepSeek-V3-0324",
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": 2000,  # Increased to handle multiple responses
        "temperature": 0.0,
        "response_format": {"type": "json_object"}  # Request JSON output if supported
    }
    
    req = urllib.request.Request(
        f"{BASE_URL}/chat/completions",
        data=json.dumps(data).encode('utf-8'),
        headers={
            "Authorization": f"Bearer {API_KEY}",
            "Content-Type": "application/json"
        },
        method="POST"
    )
    
    with urllib.request.urlopen(req) as response:
        result = json.loads(response.read().decode('utf-8'))
        content = result['choices'][0]['message']['content'].strip()
        
        # Parse the JSON response
        try:
            relationships = json.loads(content)['relationships']
        except json.JSONDecodeError:
            # Fallback: attempt to extract categories manually
            # This assumes the response might be malformed; adjust based on actual outputs
            relationships = []
            for line in content.split('\n'):
                if line.strip().startswith('"') or line.strip().isdigit():
                    relationships.append(line.strip().strip('"'))
        
        return relationships

# Process in batches to avoid exceeding context limits
batch_size = 50  # Adjust based on average paragraph length; smaller if texts are long
relationships = []
for start in range(0, len(pairs), batch_size):
    end = start + batch_size
    batch_df = pairs.iloc[start:end].reset_index(drop=True)  # Reset index for Pair numbering
    batch_rels = classify_relationships_batch(batch_df)
    if len(batch_rels) != len(batch_df):
        raise ValueError("Mismatch in number of relationships returned")
    relationships.extend(batch_rels)

# Assign the relationships to the dataframe
pairs['relationship'] = relationships

ds = SAS.df2sd(pairs, _output1)
    
endsubmit;
quit;

/* region: Generated step cleanup for LLM API Call */
%_flw_action_end(071d40d0-c2a7-11f0-8cf1-97ecc97c9d4e, PUBLIC.NEW_NEW_TEST_OUTPUT, PUBLIC, "NEW_NEW_TEST_OUTPUT");
/* endregion */

/*===========================================================================*
* Node name:        Filter Overlaps
* Node ID:          fc60c850-c2ac-11f0-808a-536665d79561
*
* Input Tables:
*   PUBLIC.NEW_NEW_TEST_OUTPUT
*
* Output Tables:
*   PUBLIC.OVERLAPS
*
* Step name:        Filter Rows
* Step path:        /dataFlows/steps/29351cc4-ab8e-40ed-804b-109f0b5a9b93
* Step description: Filter rows
*----------------------------------------------------------------------------*/

/* region: Generated step setup */

%_flw_action_start(fc60c850-c2ac-11f0-808a-536665d79561);
/* endregion */


data PUBLIC.OVERLAPS;
   set PUBLIC.NEW_NEW_TEST_OUTPUT;
   where kupcase(relationship) eq "1";
run;


/* region: Generated step cleanup for Filter Overlaps */
%_flw_action_end(fc60c850-c2ac-11f0-808a-536665d79561, PUBLIC.OVERLAPS, PUBLIC, "OVERLAPS");
/* endregion */

/*===========================================================================*
* Node name:        Filter Contradictions
* Node ID:          be6aa930-c2ac-11f0-808a-536665d79561
*
* Input Tables:
*   PUBLIC.NEW_NEW_TEST_OUTPUT
*
* Output Tables:
*   PUBLIC.CONTRADICTIONS
*
* Step name:        Filter Rows
* Step path:        /dataFlows/steps/29351cc4-ab8e-40ed-804b-109f0b5a9b93
* Step description: Filter rows
*----------------------------------------------------------------------------*/

/* region: Generated step setup */

%_flw_action_start(be6aa930-c2ac-11f0-808a-536665d79561);
/* endregion */


data PUBLIC.CONTRADICTIONS;
   set PUBLIC.NEW_NEW_TEST_OUTPUT;
   where kupcase(relationship) eq "2";
run;


/* region: Generated step cleanup for Filter Contradictions */
%_flw_action_end(be6aa930-c2ac-11f0-808a-536665d79561, PUBLIC.CONTRADICTIONS, PUBLIC, "CONTRADICTIONS");
/* endregion */

/* region: Generated flow cleanup */
%sysmacdelete _flw_action_start;
%sysmacdelete _flw_action_end;

/* endregion */

